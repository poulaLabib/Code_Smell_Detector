======================================================================
 CODE SMELL DETECTION â€” FULL PIPELINE REPORT
======================================================================

ğŸ“ PROJECT STRUCTURE
----------------------------------------------------------------------

  code_smell_project/
  â”œâ”€â”€ projects/                  â† 12 Java projects (synthetic)
  â”œâ”€â”€ ck_metrics/                â† CK metrics per project + combined JSON
  â”œâ”€â”€ sonar_issues/              â† SonarQube-style detections per project
  â”œâ”€â”€ dataset/
  â”‚   â”œâ”€â”€ dataset.csv            â† Full dataset (features + labels, no code)
  â”‚   â”œâ”€â”€ dataset.json           â† Full dataset (includes raw_code)
  â”‚   â””â”€â”€ split_info.json        â† Train/Val/Test project assignments
  â”œâ”€â”€ gold_set/
  â”‚   â”œâ”€â”€ ground_truth_meta.json â† True labels for all 191 classes
  â”‚   â””â”€â”€ gold_validation.csv    â† 156 stratified gold examples
  â”œâ”€â”€ models/
  â”‚   â”œâ”€â”€ baseline_a_results.json
  â”‚   â”œâ”€â”€ baseline_b_tfidf_results.json
  â”‚   â””â”€â”€ baseline_b_codebert.py â† Ready to run on GPU
  â”œâ”€â”€ clone_projects.sh          â† Run on YOUR machine to get real repos
  â”œâ”€â”€ generate_synthetic_java.py â† Generated the 191 Java files
  â”œâ”€â”€ ck_extractor.py            â† CK metrics extractor (pure Python)
  â”œâ”€â”€ sonar_detector.py          â† SonarQube rule emulator
  â”œâ”€â”€ build_dataset.py           â† Merges everything into dataset
  â”œâ”€â”€ baseline_a_rf.py           â† RandomForest on CK metrics
  â””â”€â”€ baseline_b_text.py         â† TF-IDF + CodeBERT pipeline


ğŸ“Š DATASET STATISTICS
----------------------------------------------------------------------
  Total classes analyzed:  191
  Projects:                12

  Smell distribution (ground truth):
    GodClass       :  30 ( 15.7%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    FeatureEnvy    :  23 ( 12.0%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    LongMethod     :  28 ( 14.7%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    DataClass      :  33 ( 17.3%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    DeadCode       :  22 ( 11.5%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    Clean          :  55 ( 28.8%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

  Project-level split (NO data leakage):
    TRAIN  (67%): 8 projects â†’ 123 examples
    VAL    ( 8%): 1 projects â†’ 17 examples
    TEST   (25%): 3 projects â†’ 51 examples

  Gold validation set: 156 stratified examples

ğŸ“ˆ BASELINE A â€” RandomForest on CK Metrics
----------------------------------------------------------------------
  Smell                    Prec      Rec       F1
  ------------------------------------------------
  GodClass                1.000    1.000    1.000
  FeatureEnvy             1.000    1.000    1.000
  LongMethod              1.000    1.000    1.000
  DataClass               1.000    1.000    1.000
  DeadCode                1.000    1.000    1.000

  Top features per smell:
    GodClass        â†’ LOC (0.29), FIELDS (0.24), METHODS (0.14)
    FeatureEnvy     â†’ ATFD (0.30), METHODS (0.19), WMC (0.17)
    LongMethod      â†’ MAX_METHOD_LOC (0.34), FIELDS (0.15), METHODS (0.14)
    DataClass       â†’ MAX_METHOD_LOC (0.31), LCOM (0.27), FIELDS (0.10)
    DeadCode        â†’ PRIVATE_METHODS (0.34), FIELDS (0.19), MAX_METHOD_LOC (0.09)

ğŸ“ˆ BASELINE B â€” TF-IDF Code Text Classifier
----------------------------------------------------------------------
  Model                            Macro F1   Micro F1
  ----------------------------------------------------
  LogisticRegression                  1.000      1.000
  RandomForest_TextFused              1.000      1.000
  MLP_TextFused                       1.000      1.000

  Ablation (LogisticRegression):
    Text-only (TF-IDF):      Macro F1 = 1.000
    Metrics-only (CK):       Macro F1 = 1.000
    Fused (TF-IDF + CK):     Macro F1 = 1.000
    â†’ On synthetic data all signals are clean.
    â†’ On REAL data, expect Text < Metrics for structural smells,
      and Fused > either alone (especially for FeatureEnvy).

âš ï¸  WHY PERFECT SCORES? (Important!)
----------------------------------------------------------------------
  Our synthetic Java files have very distinct patterns by design:
  â€¢ GodClass files have 18-30 fields and 14-25 methods
  â€¢ LongMethod files have 58-80 lines in one method
  â€¢ DataClass files have only getters/setters
  
  This makes them trivially separable. On REAL code:
  â€¢ God Classes emerge gradually â€” boundaries are fuzzy
  â€¢ Feature Envy requires understanding call semantics
  â€¢ Dead Code needs whole-program analysis
  
  Expected real-world performance:
  â€¢ LongMethod:     F1 ~ 0.85-0.95  (mostly LOC-based, straightforward)
  â€¢ GodClass:       F1 ~ 0.65-0.80  (structural, metrics help a lot)
  â€¢ DataClass:      F1 ~ 0.70-0.85  (ratio-based, works well)
  â€¢ DeadCode:       F1 ~ 0.50-0.70  (needs call-graph analysis)
  â€¢ FeatureEnvy:    F1 ~ 0.40-0.65  (most semantic, CodeBERT helps most here)


ğŸš€ NEXT STEPS â€” Running on Real Data
----------------------------------------------------------------------

  STEP 1: Clone real projects (on your machine with internet)
          bash clone_projects.sh

  STEP 2: Run CK metrics on real projects
          python ck_extractor.py
          (change BASE path to point to your real projects/)

  STEP 3: Run SonarQube (REAL Docker version) for production labels
          docker run -d --name sonarqube -p 9000:9000 sonarqube:lts
          # wait ~2 min, then run sonar-scanner on each project
          # OR keep using sonar_detector.py as a fast approximation

  STEP 4: Build dataset
          python build_dataset.py
          (manually label 200-500 examples for gold set â€” 
           replace ground_truth_meta.json with your human labels)

  STEP 5: Train baselines
          python baseline_a_rf.py      â† works anywhere
          python baseline_b_text.py    â† TF-IDF version, works anywhere

  STEP 6: Fine-tune CodeBERT (needs GPU)
          pip install transformers torch accelerate
          python baseline_b_codebert.py
          
  STEP 7: Iterate
          â€¢ If GodClass F1 < 0.7 â†’ add more training projects
          â€¢ If FeatureEnvy F1 < 0.5 â†’ CodeBERT + feature fusion is key
          â€¢ If DeadCode F1 is low â†’ consider adding call-graph features
          â€¢ Active learning: take uncertain predictions, manually label, retrain


ğŸ’¡ KEY RECOMMENDATIONS
----------------------------------------------------------------------
  1. Use MAJORITY VOTE labels (â‰¥2 tools agree) not OR â€” higher precision
  2. Always evaluate on human-labeled gold set, never trust tool-only metrics
  3. Different smells need different strategies:
     â€¢ Structural (GodClass, LongMethod) â†’ CK metrics dominate
     â€¢ Semantic (FeatureEnvy) â†’ CodeBERT embeddings are essential
     â€¢ Behavioral (DeadCode) â†’ needs call-graph or whole-program analysis
  4. Project-level splits are NON-NEGOTIABLE â€” file-level splits will lie to you
  5. Report per-smell F1 â€” macro averages hide failures on rare smells

======================================================================
 END OF REPORT
======================================================================