"""
Final Report Generator
Produces a clean summary of the entire pipeline.
"""

import json, os
from collections import Counter

BASE = "/home/claude/code_smell_project"

# Load all results
with open(os.path.join(BASE, "models", "baseline_a_results.json")) as f:
    baseline_a = json.load(f)
with open(os.path.join(BASE, "models", "baseline_b_tfidf_results.json")) as f:
    baseline_b = json.load(f)
with open(os.path.join(BASE, "dataset", "split_info.json")) as f:
    splits = json.load(f)
with open(os.path.join(BASE, "gold_set", "ground_truth_meta.json")) as f:
    truth = json.load(f)

smell_counts = Counter(t["true_smell"] for t in truth)

report = []
report.append("=" * 70)
report.append(" CODE SMELL DETECTION ‚Äî FULL PIPELINE REPORT")
report.append("=" * 70)

report.append("\nüìÅ PROJECT STRUCTURE")
report.append("-" * 70)
report.append("""
  code_smell_project/
  ‚îú‚îÄ‚îÄ projects/                  ‚Üê 12 Java projects (synthetic)
  ‚îú‚îÄ‚îÄ ck_metrics/                ‚Üê CK metrics per project + combined JSON
  ‚îú‚îÄ‚îÄ sonar_issues/              ‚Üê SonarQube-style detections per project
  ‚îú‚îÄ‚îÄ dataset/
  ‚îÇ   ‚îú‚îÄ‚îÄ dataset.csv            ‚Üê Full dataset (features + labels, no code)
  ‚îÇ   ‚îú‚îÄ‚îÄ dataset.json           ‚Üê Full dataset (includes raw_code)
  ‚îÇ   ‚îî‚îÄ‚îÄ split_info.json        ‚Üê Train/Val/Test project assignments
  ‚îú‚îÄ‚îÄ gold_set/
  ‚îÇ   ‚îú‚îÄ‚îÄ ground_truth_meta.json ‚Üê True labels for all 191 classes
  ‚îÇ   ‚îî‚îÄ‚îÄ gold_validation.csv    ‚Üê 156 stratified gold examples
  ‚îú‚îÄ‚îÄ models/
  ‚îÇ   ‚îú‚îÄ‚îÄ baseline_a_results.json
  ‚îÇ   ‚îú‚îÄ‚îÄ baseline_b_tfidf_results.json
  ‚îÇ   ‚îî‚îÄ‚îÄ baseline_b_codebert.py ‚Üê Ready to run on GPU
  ‚îú‚îÄ‚îÄ clone_projects.sh          ‚Üê Run on YOUR machine to get real repos
  ‚îú‚îÄ‚îÄ generate_synthetic_java.py ‚Üê Generated the 191 Java files
  ‚îú‚îÄ‚îÄ ck_extractor.py            ‚Üê CK metrics extractor (pure Python)
  ‚îú‚îÄ‚îÄ sonar_detector.py          ‚Üê SonarQube rule emulator
  ‚îú‚îÄ‚îÄ build_dataset.py           ‚Üê Merges everything into dataset
  ‚îú‚îÄ‚îÄ baseline_a_rf.py           ‚Üê RandomForest on CK metrics
  ‚îî‚îÄ‚îÄ baseline_b_text.py         ‚Üê TF-IDF + CodeBERT pipeline
""")

report.append("\nüìä DATASET STATISTICS")
report.append("-" * 70)
report.append("  Total classes analyzed:  191")
report.append("  Projects:                12")
report.append("")
report.append("  Smell distribution (ground truth):")
for smell in ["GodClass", "FeatureEnvy", "LongMethod", "DataClass", "DeadCode", "Clean"]:
    count = smell_counts.get(smell, 0)
    pct = count / 191 * 100
    bar = "‚ñà" * int(pct / 2)
    report.append("    {:15s}: {:3d} ({:5.1f}%)  {}".format(smell, count, pct, bar))

report.append("")
report.append("  Project-level split (NO data leakage):")
report.append("    TRAIN  (67%): {} projects ‚Üí {} examples".format(
    len(splits["train"]), sum(1 for t in truth if t["project"] in splits["train"])))
report.append("    VAL    ( 8%): {} projects ‚Üí {} examples".format(
    len(splits["val"]), sum(1 for t in truth if t["project"] in splits["val"])))
report.append("    TEST   (25%): {} projects ‚Üí {} examples".format(
    len(splits["test"]), sum(1 for t in truth if t["project"] in splits["test"])))

report.append("\n  Gold validation set: 156 stratified examples")

report.append("\nüìà BASELINE A ‚Äî RandomForest on CK Metrics")
report.append("-" * 70)
report.append("  {:20s} {:>8s} {:>8s} {:>8s}".format("Smell", "Prec", "Rec", "F1"))
report.append("  " + "-" * 48)
SMELL_TYPES = ["GodClass", "FeatureEnvy", "LongMethod", "DataClass", "DeadCode"]
for smell in SMELL_TYPES:
    key = smell + "__RandomForest"
    if key in baseline_a["results"]:
        r = baseline_a["results"][key]
        report.append("  {:20s} {:>8.3f} {:>8.3f} {:>8.3f}".format(
            smell, r["precision"], r["recall"], r["f1"]))

report.append("")
report.append("  Top features per smell:")
for smell in SMELL_TYPES:
    if smell in baseline_a["feature_importances"]:
        fi = baseline_a["feature_importances"][smell]
        top = sorted(fi.items(), key=lambda x: -x[1])[:3]
        top_str = ", ".join(["{} ({:.2f})".format(f, v) for f, v in top])
        report.append("    {:15s} ‚Üí {}".format(smell, top_str))

report.append("\nüìà BASELINE B ‚Äî TF-IDF Code Text Classifier")
report.append("-" * 70)
report.append("  {:30s} {:>10s} {:>10s}".format("Model", "Macro F1", "Micro F1"))
report.append("  " + "-" * 52)
for model_name, r in baseline_b.items():
    report.append("  {:30s} {:>10.3f} {:>10.3f}".format(
        model_name, r["macro_f1"], r["micro_f1"]))

report.append("")
report.append("  Ablation (LogisticRegression):")
report.append("    Text-only (TF-IDF):      Macro F1 = 1.000")
report.append("    Metrics-only (CK):       Macro F1 = 1.000")
report.append("    Fused (TF-IDF + CK):     Macro F1 = 1.000")
report.append("    ‚Üí On synthetic data all signals are clean.")
report.append("    ‚Üí On REAL data, expect Text < Metrics for structural smells,")
report.append("      and Fused > either alone (especially for FeatureEnvy).")

report.append("\n‚ö†Ô∏è  WHY PERFECT SCORES? (Important!)")
report.append("-" * 70)
report.append("""  Our synthetic Java files have very distinct patterns by design:
  ‚Ä¢ GodClass files have 18-30 fields and 14-25 methods
  ‚Ä¢ LongMethod files have 58-80 lines in one method
  ‚Ä¢ DataClass files have only getters/setters
  
  This makes them trivially separable. On REAL code:
  ‚Ä¢ God Classes emerge gradually ‚Äî boundaries are fuzzy
  ‚Ä¢ Feature Envy requires understanding call semantics
  ‚Ä¢ Dead Code needs whole-program analysis
  
  Expected real-world performance:
  ‚Ä¢ LongMethod:     F1 ~ 0.85-0.95  (mostly LOC-based, straightforward)
  ‚Ä¢ GodClass:       F1 ~ 0.65-0.80  (structural, metrics help a lot)
  ‚Ä¢ DataClass:      F1 ~ 0.70-0.85  (ratio-based, works well)
  ‚Ä¢ DeadCode:       F1 ~ 0.50-0.70  (needs call-graph analysis)
  ‚Ä¢ FeatureEnvy:    F1 ~ 0.40-0.65  (most semantic, CodeBERT helps most here)
""")

report.append("\nüöÄ NEXT STEPS ‚Äî Running on Real Data")
report.append("-" * 70)
report.append("""
  STEP 1: Clone real projects (on your machine with internet)
          bash clone_projects.sh

  STEP 2: Run CK metrics on real projects
          python ck_extractor.py
          (change BASE path to point to your real projects/)

  STEP 3: Run SonarQube (REAL Docker version) for production labels
          docker run -d --name sonarqube -p 9000:9000 sonarqube:lts
          # wait ~2 min, then run sonar-scanner on each project
          # OR keep using sonar_detector.py as a fast approximation

  STEP 4: Build dataset
          python build_dataset.py
          (manually label 200-500 examples for gold set ‚Äî 
           replace ground_truth_meta.json with your human labels)

  STEP 5: Train baselines
          python baseline_a_rf.py      ‚Üê works anywhere
          python baseline_b_text.py    ‚Üê TF-IDF version, works anywhere

  STEP 6: Fine-tune CodeBERT (needs GPU)
          pip install transformers torch accelerate
          python baseline_b_codebert.py
          
  STEP 7: Iterate
          ‚Ä¢ If GodClass F1 < 0.7 ‚Üí add more training projects
          ‚Ä¢ If FeatureEnvy F1 < 0.5 ‚Üí CodeBERT + feature fusion is key
          ‚Ä¢ If DeadCode F1 is low ‚Üí consider adding call-graph features
          ‚Ä¢ Active learning: take uncertain predictions, manually label, retrain
""")

report.append("\nüí° KEY RECOMMENDATIONS")
report.append("-" * 70)
report.append("""  1. Use MAJORITY VOTE labels (‚â•2 tools agree) not OR ‚Äî higher precision
  2. Always evaluate on human-labeled gold set, never trust tool-only metrics
  3. Different smells need different strategies:
     ‚Ä¢ Structural (GodClass, LongMethod) ‚Üí CK metrics dominate
     ‚Ä¢ Semantic (FeatureEnvy) ‚Üí CodeBERT embeddings are essential
     ‚Ä¢ Behavioral (DeadCode) ‚Üí needs call-graph or whole-program analysis
  4. Project-level splits are NON-NEGOTIABLE ‚Äî file-level splits will lie to you
  5. Report per-smell F1 ‚Äî macro averages hide failures on rare smells
""")

report.append("=" * 70)
report.append(" END OF REPORT")
report.append("=" * 70)

full_report = "\n".join(report)
print(full_report)

# Save report
with open(os.path.join(BASE, "REPORT.txt"), "w") as f:
    f.write(full_report)
print("\n‚úì Report saved to REPORT.txt")
